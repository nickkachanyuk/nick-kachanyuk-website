---
title: "The Data Behind the Tanzanian Water Crisis"
author: "Nick Kachanyuk"
date: '2021-08-02'
slug: []
categories: R
tags:
- data science
- R
meta_img: images/image.png
description: Description for the page
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p><img src="images/lions.png" height="500" /></p>
<div id="overview-of-this-blog-post" class="section level1">
<h1>Overview of this blog post</h1>
<p>According to Water.org[1], Tanzania is experiencing a water and sanitation crisis. According to their records 4 million people lack access to safe water resources. People often spend a significant amount of time traveling long distances to collect water, and this burden often falls on women and girls. Using the data provided for the Pump it Up competition by DrivenData[2] along with Tanzania’s 2012 census report[3] we will go through the machine learning approach using XGBoost to create a model that can predict the whether a water well is functional, needs repair, or non-functional. In addition, we will explore the features that were used in the model.</p>
<div id="background" class="section level2">
<h2>Background</h2>
<p>Earlier this year, DrivenData hosted an educational data science competition that requires a machine learning approach that helps identify Tanzania’s water pump functionality. The data provided for the competition contains variables that focus on the specific features of these water wells such as location of the well, construction year, record date, and quality of the water. Identifying the functionality of water wells in Tanzania is important in addressing the water crisis in Tanzania but there is a greater opportunity to explore additional socioeconomic factors that may show why a particular water shortage is occurring in each region of the country.</p>
</div>
<div id="what-is-the-problem" class="section level2">
<h2>What is the problem?</h2>
<p>Tanzania is experiencing a water crisis where about 4 million people lack access to safe water resources. While there is data on both Tanzania’s water well quality and the 2012 census, there is no attempt to combine these resources to assess the overall water situation in the country. Solving the water crisis requires both a problem-specific approach (identifying functionality of the wells) and a broader analysis of how different regions in the country are affected. Although there is no clear cause and effect pattern, identifying which regions are doing well in relation to others may help develop preventative strategies for the future and can also shed light on the challenges that the citizens of this country are facing when trying to access safe drinking water.</p>
</div>
<div id="why-is-it-important-to-solve" class="section level2">
<h2>Why is it important to solve?</h2>
<p>In 2016, Water.org found that Tanzania is eligible for what they call a “water credit solution” which allows for lending programs to households and water companies. My project attempts at providing practical guidelines in solving the water crisis by first addressing the functionality of the water wells for each region in the country. Combining socioeconomic data into the analysis may also help answer questions such as how the water crisis is persistent for a given region. Examining the socioeconomic differences between regions may help get at issues like, which regions are experiencing water access and quality issues the most.</p>
</div>
</div>
<div id="the-data" class="section level1">
<h1>The Data</h1>
<div id="outcome-variable-in-the-model" class="section level2">
<h2>Outcome variable in the model</h2>
<p>The dependent variable in the model is called status_group. This is a categorical variable that contains three classes functional, functional needs repair (FNR), and non-functional. Since this is a categorical variable, a supervised machine learning classification approach will be used. The best model will be selected via the highest Cohen’s kappa score.</p>
<p>Kappa is about how much better your model classifier is performing over the performance of a classifier that simply guesses at random according to the frequency of each class. Kappa takes on a range of values from -1 to 1.</p>
<p>According [Landis, J.R.; Koch, G.G. (1977). “The measurement of observer agreement for categorical data”. Biometrics 33 (1): 159–174], Kappa scores can be categorized as follows:
less than 0.20 (not good)
0.21 to 0.40 (fair)
0.41 to 0.60 (pretty good)
0.61 to 0.80 (great)
greater than 0.80 (almost perfect)</p>
</div>
<div id="what-types-of-features-were-considered" class="section level2">
<h2>What types of features were considered?</h2>
<p>The features and their descriptions from the Pump it Up data can be found here:
[<a href="https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/25/" class="uri">https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/25/</a>]</p>
<p>The features from the Tanzania 2012 census are:
total_region_pop: total population for a given Tanzanian region
households_per_region: number of households by region
mhead_avg_household_size_per_region: average number of male head of household by region
fhead_avg_household_size_per_region: average number of female head of household by region
literacy_rate_by_region: percent of the literate population by region
pct_unemployed_by_region: percent of the unemployed population by region
region_area_sq_mi: area in square miles of a given region</p>
<p>Before I get into the final model and results. I would like to briefly mention a few things about the data challenges.</p>
</div>
<div id="dealing-with-missing-data" class="section level2">
<h2>Dealing with missing data</h2>
<p>Here is a plot of the missing data:</p>
<p><img src="images/missing_data.png" height="500" /></p>
<p>The data appears to not be missing at random because if we look at installer and funder variables, there is a 100% match in the rows where the data is missing. In addition, there were 4 regions in which all the observations had a zero value for amount_tsh variable. An imputation strategy is needed because we can not simply delete the rows where the data is missing because the data is not missing at random. I wont elaborate too much on this in this post, because it is a whole process of its own, but the code for this entire project can be found in my GitHub repo [].</p>
<p>Looking at the missing values graph we can exclude variables like scheme_name, scheme_management, installer, and funder from further analysis. The wpt_name is a unique identifying name value for each observation and can also be excluded from the analysis.</p>
</div>
<div id="target-variable-class-imbalance-or-unnecessary-classes-present" class="section level2">
<h2>Target variable: class imbalance or unnecessary classes present?</h2>
<p>Class imbalance is an important topic to discuss when preparing data for predictive modeling. From the graph below we can see that the functional class is the most popular class in the target variable (status_group).</p>
<pre><code>## Warning: Missing column names filled in: &#39;X1&#39; [1]</code></pre>
<pre><code>## # A tibble: 3 x 2
##   status_group            well_count
##   &lt;fct&gt;                        &lt;int&gt;
## 1 functional                   32259
## 2 functional needs repair       4317
## 3 non functional               22824</code></pre>
<p>Class imbalance can be problematic for some classification problems because a model can become ‘fixed’ on the most popular class (because it is most frequent in the data) and fail to learn as much about the smaller classes thus decreasing the accuracy in the predictions. A solid strategy is to upscale a less frequent class by generating more observations that resemble that class.</p>
<p>Another possible approach is to collapse FNR class with either functional or non-functional category. This will result in only 2 factor levels for the target variable, decrease the gap in the class imbalance, and also impose a more general classification system. If a pump needs repair it is likely to become non-functional in the near future and I think it makes sense to have two classes that could be: functional pumps, and pumps that need to fixed. This becomes more evident as a discuss the final features and how well the three classes are distributed among them.</p>
</div>
</div>
<div id="the-model" class="section level1">
<h1>The model</h1>
<p>For this project I chose to focus on the XGBoost model. The data has several outliers which XGBoost handles well. XGBoost is also good for large sized datasets that other models take longer to process and is less prone to overfitting thus making it notorious for good model performance.</p>
<p>On the other hand, XGBoost model is difficult to interpret when the tree plot has too many nodes (which was the case for my best performing model). It is also a bit more challenging to tune due to having many hyperparameters.</p>
<div id="hyperparameter-tuning" class="section level2">
<h2>Hyperparameter tuning</h2>
<p>XGBoost tree model has the following hyperparameters [<a href="https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/" class="uri">https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/</a>]:</p>
<p>I selected what seemed to be most appropriate based on the defintions of those variable provided and also using cross-validation during the model tuning.</p>
<p>Below are hyperparameter values of the best performing XGBoost model and also a visual representation of hyperparameter tune grid.</p>
<pre><code>##    nrounds max_depth  eta gamma colsample_bytree min_child_weight subsample
## 61    1000         8 0.05     0              0.9                5       0.5</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>It seems classifiers with higher values of max_depth perform better than more shallow tree classifiers, and the lower to intermediate eta values are a good combination as well.</p>
<p>Next, let’s examine the confusion matrix.</p>
</div>
<div id="confusion-matrix" class="section level2">
<h2>Confusion matrix</h2>
<pre><code>## Confusion Matrix and Statistics
## 
##                          Reference
## Prediction                functional functional needs repair non functional
##   functional                    7101                     608           1366
##   functional needs repair        206                     307             72
##   non functional                 764                     142           4284
## 
## Overall Statistics
##                                           
##                Accuracy : 0.7873          
##                  95% CI : (0.7807, 0.7939)
##     No Information Rate : 0.5435          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.599           
##                                           
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
## 
## Statistics by Class:
## 
##                      Class: functional Class: functional needs repair
## Sensitivity                     0.8798                        0.29044
## Specificity                     0.7088                        0.97984
## Pos Pred Value                  0.7825                        0.52479
## Neg Pred Value                  0.8320                        0.94742
## Prevalence                      0.5435                        0.07118
## Detection Rate                  0.4782                        0.02067
## Detection Prevalence            0.6111                        0.03939
## Balanced Accuracy               0.7943                        0.63514
##                      Class: non functional
## Sensitivity                         0.7487
## Specificity                         0.9007
## Pos Pred Value                      0.8254
## Neg Pred Value                      0.8511
## Prevalence                          0.3853
## Detection Rate                      0.2885
## Detection Prevalence                0.3495
## Balanced Accuracy                   0.8247</code></pre>
<p>The model is classifies functional wells better than any other target class. It seems to struggle with classifying FNR wells and does a mediocre job at classifying non-functional wells. Collapsing FNR and non-functional class may seem like a good suggestion for further analysis considerations.</p>
<p>The Kappa value of 0.599 shows that this is a good classifier after all and it seems like there is no apparent overfitting.</p>
</div>
<div id="examining-the-improtance-of-features-in-the-model-and-feature-characterstics-distribution-suggestions-flaws-etc" class="section level2">
<h2>Examining the improtance of features in the model and feature characterstics (distribution, suggestions, flaws, etc…)</h2>
<div id="feature-importance" class="section level3">
<h3>Feature importance</h3>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>The top four most important variables in the model are all feature engineered variables that were created via PCA (principal component analysis). This is probably because they contain a good proportion of the variance in the data. Surprisingly the original categorical variable “permit” is the least predictive even though one might believe that wells that have a permit are more likely to be functional because they have undergone more inspection and compliance.</p>
<p>Let’s examine each variable more closely now.</p>
</div>
<div id="amount-tsh-variable" class="section level3">
<h3>Amount tsh variable</h3>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>The variable amount_tsh is the original variable in the Tanzania water well data. The description of this variable reads “total static head (amount water available to waterpoint)” with no units of measurement specified. Either this variable can be dropped for a decrease in variance but hopefully improving the accuracy after using a new model or examining the box plot which shows that it is a reasonable variable to use afterall. If we keep the outliers, which XGBoost handles well, they can be used to show the differences between the three target groups. It adds up, function wells will indeed have more water avaialbe to waterpoint than any other two categories. Once again though, the argument for collapsing functional needs repair class and non functional class is once again maybe a purposeful suggestion to try out.</p>
</div>
<div id="literacry-rate-by-region-variable" class="section level3">
<h3>Literacry rate by region variable</h3>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>According to the VIP plot, literacy rate by region is in 9/12th position for importance which is also evident by how the IQR ranges overlap for each target category. Once again collapsing FNR(functional needs repair) with non functional is once again proposed as an avenue to explore.</p>
</div>
<div id="perecent-unemployed-by-region" class="section level3">
<h3>Perecent unemployed by region</h3>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>According to VIP plot, this variable is 10/12th place for importance. There also seems to be a pattern for how importance is measured. Reading [“<a href="https://cran.r-project.org/web/packages/vip/vignettes/vip-introduction.pdf" class="uri">https://cran.r-project.org/web/packages/vip/vignettes/vip-introduction.pdf</a>”].</p>
<p>“For classification problems, an area under the ROC curve (AUC) statistic can be used to quantify predictor importance.
The AUC statistic is computed by using the predictor x as input to the ROC curve. If x can reasonably
separate the classes of Y, that is a clear indicator that x is an important predictor (in terms of class
separation) and this is captured in the corresponding AUC statistic.”</p>
<p>The target variable does not separate well in this predictor. This variable could be further dropped from analysis which may improve the accuracy of the model.</p>
</div>
<div id="well-strain-variable" class="section level3">
<h3>Well strain variable</h3>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>This variable is 6/12th position on importance plot. Well strain is a variable that was feature engineered using the following formula:</p>
<p>well_strain = (population/total_region_pop) * region_pop_density)
- where population is the number of people living around the well
- total_region_pop is the total number of people living in a given region of Tanzania
- region_pop_density is total_region_pop/region_area_sq_mi</p>
<p>I wonder if there is any difference in the IQR ranges of the well strain variable between the different status groups?</p>
<pre><code>## # A tibble: 3 x 2
##   status_group               iqr
##   &lt;fct&gt;                    &lt;dbl&gt;
## 1 functional              0.0185
## 2 functional needs repair 0.0144
## 3 non functional          0.0185</code></pre>
<p>100% overlap between functional and non functional groups.</p>
</div>
<div id="permit-variable-de-dummified" class="section level3">
<h3>Permit variable de-dummified</h3>
<p><img src="images/permit.dist.png" height="500" /></p>
<p>From the density plot we can see that the permit variable distribution does not separate well between the target classes and probably the reason behind the dummy versions scoring 11th and last place on the importance graph.</p>
</div>
<div id="handpump-groundwater-shallow-well" class="section level3">
<h3>Handpump groundwater shallow well</h3>
<p>Visualize it.
Talk about how it was feature engineered via pca.
Talk about how much variance it captured.</p>
</div>
</div>
</div>
